{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71cb104c-588c-4a0d-b888-6c8449a2bc01",
   "metadata": {},
   "source": [
    "# Tokenizing and Translating Markdown with OpenAI\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement a pipeline to tokenize a markdown document, split it into chunks at multiple newlines, translate those chunks using OpenAI's LLMs, and reconstruct the full document while retaining original formatting.\n",
    "\n",
    "Specifically, we will:\n",
    "\n",
    "- Read in a markdown file \n",
    "- Tokenize the text into words, punctuation, etc.\n",
    "- Count the number of tokens\n",
    "- Split the tokens into chunks whenever there are multiple successive newlines \n",
    "- Translate each chunk into another language using OpenAI's translation LLMs\n",
    "- Reconstruct the translated chunks into a full document, preserving original formatting like headers, lists, etc.\n",
    "\n",
    "This allows us to get translations while keeping code blocks, images, tables intact. \n",
    "\n",
    "We'll use Python and Jupyter notebooks to implement the pipeline. The notebook will be structured into sections for each step of the process.\n",
    "\n",
    "To follow along, you'll want a markdown file to process. We'll use a small sample file included with the notebook. You'll need access to OpenAI's API.\n",
    "\n",
    "Let's get started! First we'll import the modules and setup the notebook. Then we'll define functions for each step - tokenizing, counting tokens, splitting, translating, and reconstructing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5231a58-0458-473d-a802-4f5f0caafe94",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2c0fa-a4be-4ab7-89cc-0e9a692e0660",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201397c-c5e3-4229-934a-57b640b7d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dbb258-e065-4a27-9296-d9e346b49cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e7129-7ae8-4b4c-8a67-c108aa531cf2",
   "metadata": {},
   "source": [
    "### Import Input File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6c37e-812d-4191-82a1-3080d5b0fb6a",
   "metadata": {},
   "source": [
    "Import the data file you want translated from `data/input.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1cf352f-2246-4aee-bd86-1c034cab0afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d121723-ed09-48b0-a7c5-2441eae8526c",
   "metadata": {},
   "source": [
    "Length of the text file (for our purposes, we'll be using a small sample text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac60e6b-43bb-4fc2-99ac-3f1f975a6c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ddad7-463e-4e3c-ac61-7b6305c7683b",
   "metadata": {},
   "source": [
    "### Counting the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66a822-dddb-4eda-8ce5-f148461d3d8e",
   "metadata": {},
   "source": [
    "We'll use OpenAI's `tiktoken` - a fast open-source tokenizer:\n",
    "\n",
    "> Given a text string (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), a tokenizer can split the text string into a list of tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
    "\n",
    "> Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "effb6066-1756-4e1f-af01-497db122aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "encoded_text = encoding.encode(text)\n",
    "input_token_count = len(encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b67afe42-81e7-4b3d-b8e9-ea60f37aaecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1062"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6bf65-0b10-48f0-a5a1-d8f5355c78d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
